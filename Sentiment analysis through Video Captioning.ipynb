{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6896757,"sourceType":"datasetVersion","datasetId":3960795},{"sourceId":4856157,"sourceType":"datasetVersion","datasetId":2815073}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# MSVD Video Captioning Pipeline (Starter Code)\n# Phase 1: Load Annotations and Preprocess Captions\n\nimport pandas as pd\nimport os\nimport re\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Load English annotations\nANNOTATION_PATH = \"/kaggle/input/msvd-dataset-corpus/annotations.txt\"\nannotations = {}\nwith open(ANNOTATION_PATH, \"r\") as file:\n    for line in file:\n        video_id, caption = line.strip().split(' ', 1)\n        annotations.setdefault(video_id, []).append(caption.lower())\n\nprint(f\"Loaded captions for {len(annotations)} video segments\")\n\n# Add special tokens and clean captions\ndef preprocess_caption(caption):\n    caption = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", caption)\n    return f\"startseq {caption.strip()} endseq\"\n\n# Apply preprocessing\ndataset = []\nfor video_id, caps in annotations.items():\n    for c in caps:\n        dataset.append((video_id, preprocess_caption(c)))\n\nprint(f\"Total caption pairs: {len(dataset)}\")\n\n# Create tokenizer\nall_captions = [cap for _, cap in dataset]\ntokenizer = Tokenizer(oov_token='<unk>')\ntokenizer.fit_on_texts(all_captions)\nvocab_size = len(tokenizer.word_index) + 1\nprint(f\"Vocabulary size: {vocab_size}\")\n\n# Create a dictionary of video_id -> processed captions\ndescriptions = {}\nfor vid, cap in dataset:\n    descriptions.setdefault(vid, []).append(cap)\n\n# Save tokenizer for later\nimport pickle\nwith open(\"tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tokenizer, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:01:17.515460Z","iopub.execute_input":"2025-05-11T17:01:17.515744Z","iopub.status.idle":"2025-05-11T17:01:18.418514Z","shell.execute_reply.started":"2025-05-11T17:01:17.515721Z","shell.execute_reply":"2025-05-11T17:01:18.417740Z"}},"outputs":[{"name":"stdout","text":"Loaded captions for 1970 video segments\nTotal caption pairs: 80827\nVocabulary size: 12596\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Phase 2: Frame Extraction + CNN Feature Extraction\nimport cv2\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import img_to_array\n\n# Load InceptionV3 model without top layer\nbase_model = InceptionV3(weights='imagenet')\ncnn_model = Model(base_model.input, base_model.layers[-2].output)\n\nVIDEO_DIR = \"/kaggle/input/msvd-clips/YouTubeClips\"\n\n# Function to extract frames from a video\ndef extract_frames(video_path, num_frames=5):\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_ids = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n    frames = []\n\n    for fid in frame_ids:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, fid)\n        success, frame = cap.read()\n        if success:\n            frame = cv2.resize(frame, (299, 299))\n            frame = img_to_array(frame)\n            frame = preprocess_input(frame)\n            frames.append(frame)\n    cap.release()\n    return np.array(frames)\n\n# Function to extract features for a video\ndef extract_video_features(video_id):\n    video_path = os.path.join(VIDEO_DIR, video_id + \".avi\")\n    if not os.path.exists(video_path):\n        return None\n    frames = extract_frames(video_path)\n    if len(frames) == 0:\n        return None\n    features = cnn_model.predict(frames, verbose=0)\n    return np.mean(features, axis=0)\n\n# Example: Extract and save features for first 10 videos\nvideo_features = {}\nfor i, video_id in enumerate(list(descriptions.keys())[:10]):\n    feats = extract_video_features(video_id)\n    if feats is not None:\n        video_features[video_id] = feats\n    print(f\"[{i+1}/10] Processed: {video_id}\")\n\n# Save extracted features\nwith open(\"video_features.pkl\", \"wb\") as f:\n    pickle.dump(video_features, f)\n\nprint(\"Saved video features for 10 videos.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:01:32.114546Z","iopub.execute_input":"2025-05-11T17:01:32.114798Z","iopub.status.idle":"2025-05-11T17:01:41.821639Z","shell.execute_reply.started":"2025-05-11T17:01:32.114781Z","shell.execute_reply":"2025-05-11T17:01:41.820997Z"}},"outputs":[{"name":"stdout","text":"[1/10] Processed: -4wsuPCjDBc_5_15\n[2/10] Processed: -7KMZQEsJW4_205_208\n[3/10] Processed: -8y1Q0rA3n8_108_115\n[4/10] Processed: -8y1Q0rA3n8_95_102\n[5/10] Processed: -9CUm-2cui8_39_44\n[6/10] Processed: -AwoiGR6c8M_10_14\n[7/10] Processed: -Cv5LsqKUXc_17_25\n[8/10] Processed: -Cv5LsqKUXc_71_76\n[9/10] Processed: -DKuLXYoY3g_14_20\n[10/10] Processed: -DRy7rBg0IQ_31_37\nSaved video features for 10 videos.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"\n# Phase 3: Caption Generation Model\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n\n# Parameters\nmax_length = max(len(c.split()) for c in all_captions)\nembedding_dim = 256\n\n# Define the captioning model\ndef define_model(vocab_size, max_length):\n    # Feature extractor (from video)\n    inputs1 = Input(shape=(2048,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(embedding_dim, activation='relu')(fe1)\n\n    # Sequence processor (captions)\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)\n    # se3 = LSTM(256)(se2)\n    se3 = LSTM(256, return_sequences=False, recurrent_activation='sigmoid')(se2)\n    # Decoder (combine both)\n    decoder1 = add([fe2, se3])\n    decoder2 = Dense(256, activation='relu')(decoder1)\n    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    return model\n\n# Create the model\nmodel = define_model(vocab_size, max_length)\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:02:32.556259Z","iopub.execute_input":"2025-05-11T17:02:32.556794Z","iopub.status.idle":"2025-05-11T17:02:32.776871Z","shell.execute_reply.started":"2025-05-11T17:02:32.556767Z","shell.execute_reply":"2025-05-11T17:02:32.776274Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_7             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ input_layer_6             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │      \u001b[38;5;34m3,224,576\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m524,544\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m525,312\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │                        │                │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add (\u001b[38;5;33mAdd\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n│                           │                        │                │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m65,792\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12596\u001b[0m)          │      \u001b[38;5;34m3,237,172\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_7             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ input_layer_6             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,224,576</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │                        │                │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n│                           │                        │                │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12596</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,237,172</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,577,396\u001b[0m (28.91 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,577,396</span> (28.91 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,577,396\u001b[0m (28.91 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,577,396</span> (28.91 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Phase 4: Training + Caption Generation + Evaluation \n\nimport numpy as np\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom nltk.translate.bleu_score import corpus_bleu\n\n# Step 1: Create training sequences (with right-padding fix)\ndef create_sequences(tokenizer, max_length, descriptions, video_features, vocab_size):\n    X1, X2, y = [], [], []\n    for video_id, caps in descriptions.items():\n        feature = video_features.get(video_id)\n        if feature is None:\n            continue\n        for cap in caps:\n            seq = tokenizer.texts_to_sequences([cap])[0]\n            for i in range(1, len(seq)):\n                in_seq, out_seq = seq[:i], seq[i]\n                # ✅ Fix: ensure right-padding for cuDNN compatibility\n                in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                X1.append(feature)\n                X2.append(in_seq)\n                y.append(out_seq)\n    return np.array(X1), np.array(X2), np.array(y)\n\n# Prepare sequences\nX1train, X2train, ytrain = create_sequences(tokenizer, max_length, descriptions, video_features, vocab_size)\nprint(f\"Training data shapes -> Video: {X1train.shape}, Captions: {X2train.shape}, Labels: {ytrain.shape}\")\n\n# Step 2: Train the model\nhistory = model.fit([X1train, X2train], ytrain, epochs=10, batch_size=64, verbose=1)\n\n# Save the trained model\n# model.save(\"video_caption_model.h5\")\n\nmodel.export(\"video_caption_model\")\n\nprint(\"Model saved!\")\n\n\n# Step 3: Generate caption (inference, fixed version)\ndef generate_caption(model, tokenizer, video_feat, max_length):\n    in_text = 'startseq'\n    for _ in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        # ✅ Fix: pad with right-padding (post), for cuDNN compliance\n        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n        yhat = model.predict([np.array([video_feat]), sequence], verbose=0)\n        yhat = np.argmax(yhat)\n        word = tokenizer.index_word.get(yhat)\n        if word is None:\n            break\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n    return in_text.replace('startseq', '').replace('endseq', '').strip()\n\n# Example caption generation\ntest_video_id = list(video_features.keys())[0]\ntest_feat = video_features[test_video_id]\ncaption = generate_caption(model, tokenizer, test_feat, max_length)\nprint(f\"Generated Caption for video {test_video_id}:\\n{caption}\")\n\n# Step 4: BLEU score evaluation (safe against future key errors)\nactual, predicted = [], []\nfor vid_id in list(descriptions.keys())[:10]:  # Evaluate on 10 samples\n    y_pred = generate_caption(model, tokenizer, video_features[vid_id], max_length)\n    references = [d.split() for d in descriptions[vid_id]]\n    actual.append(references)\n    predicted.append(y_pred.split())\n\n# BLEU scores\nprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\nprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\nprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\nprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:31:02.077821Z","iopub.execute_input":"2025-05-11T17:31:02.078130Z","iopub.status.idle":"2025-05-11T17:31:13.997836Z","shell.execute_reply.started":"2025-05-11T17:31:02.078108Z","shell.execute_reply":"2025-05-11T17:31:13.997016Z"}},"outputs":[{"name":"stdout","text":"Training data shapes -> Video: (2950, 2048), Captions: (2950, 47), Labels: (2950, 12596)\nEpoch 1/10\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.7820\nEpoch 2/10\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7856 \nEpoch 3/10\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7227\nEpoch 4/10\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7297\nEpoch 5/10\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7059\nEpoch 6/10\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7120\nEpoch 7/10\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6856\nEpoch 8/10\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6376\nEpoch 9/10\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6565\nEpoch 10/10\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6202\nSaved artifact at 'video_caption_model'. The following endpoints are available:\n\n* Endpoint 'serve'\n  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 2048), dtype=tf.float32, name='keras_tensor_1255'), TensorSpec(shape=(None, 47), dtype=tf.float32, name='keras_tensor_1258')]\nOutput Type:\n  TensorSpec(shape=(None, 12596), dtype=tf.float32, name=None)\nCaptures:\n  135079144467408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  135075921499088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  135079144468560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  135079144470096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  135079144467792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  135079144470288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  135079144469136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  135075923516048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  135075923516624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  135075923518928: TensorSpec(shape=(), dtype=tf.resource, name=None)\nModel saved!\nGenerated Caption for video -4wsuPCjDBc_5_15:\na squirrel is eating a peanut\nBLEU-1: 1.000000\nBLEU-2: 1.000000\nBLEU-3: 0.994408\nBLEU-4: 0.977920\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# ✅ Phase 5 (Part 1): Sentiment Analysis on Generated Captions\n\n# Step 1: Install Transformers library\n!pip install transformers\n\n# Step 2: Load Sentiment Model\nfrom transformers import pipeline\n\n# Load HuggingFace Sentiment Classifier\nsentiment_analyzer = pipeline(\"sentiment-analysis\")\n\n# Step 3: Generate Caption (Ensure padding is right-padded)\ndef generate_caption(model, tokenizer, video_feat, max_length):\n    in_text = 'startseq'\n    for _ in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n        yhat = model.predict([np.array([video_feat]), sequence], verbose=0)\n        yhat = np.argmax(yhat)\n        word = tokenizer.index_word.get(yhat)\n        if word is None:\n            break\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n    return in_text.replace('startseq', '').replace('endseq', '').strip()\n\n# Step 4: Analyze Sentiment Function\ndef analyze_caption_sentiment(caption):\n    result = sentiment_analyzer(caption)[0]\n    label = result['label']\n    score = round(result['score'], 3)\n    return label, score\n\n# Step 5: Run on Example Video\ntest_video_id = list(video_features.keys())[0]\ntest_feat = video_features[test_video_id]\n\ncaption = generate_caption(model, tokenizer, test_feat, max_length)\nlabel, score = analyze_caption_sentiment(caption)\n\nprint(f\"Generated Caption: {caption}\")\nprint(f\"Sentiment: {label} (Confidence: {score})\")\n\n# Step 6: Batch Process Multiple Videos (Optional)\nfor vid_id in list(video_features.keys())[:10]:\n    test_feat = video_features[vid_id]\n    caption = generate_caption(model, tokenizer, test_feat, max_length)\n    label, score = analyze_caption_sentiment(caption)\n    print(f\"Video: {vid_id}\")\n    print(f\"  Caption: {caption}\")\n    print(f\"  Sentiment: {label} (Confidence: {score})\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:32:22.269148Z","iopub.execute_input":"2025-05-11T17:32:22.269457Z","iopub.status.idle":"2025-05-11T17:32:31.273002Z","shell.execute_reply.started":"2025-05-11T17:32:22.269433Z","shell.execute_reply":"2025-05-11T17:32:31.272313Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Generated Caption: a squirrel is eating a peanut\nSentiment: POSITIVE (Confidence: 0.582)\nVideo: -4wsuPCjDBc_5_15\n  Caption: a squirrel is eating a peanut\n  Sentiment: POSITIVE (Confidence: 0.582)\n\nVideo: -7KMZQEsJW4_205_208\n  Caption: a man is holding dead sunflowers\n  Sentiment: NEGATIVE (Confidence: 0.974)\n\nVideo: -8y1Q0rA3n8_108_115\n  Caption: a man is cutting a bottle of water with a sword\n  Sentiment: NEGATIVE (Confidence: 0.875)\n\nVideo: -8y1Q0rA3n8_95_102\n  Caption: a man is stabbing a cardboard cutout with a sword\n  Sentiment: NEGATIVE (Confidence: 0.998)\n\nVideo: -9CUm-2cui8_39_44\n  Caption: a woman is boiling finger in a pot\n  Sentiment: NEGATIVE (Confidence: 0.952)\n\nVideo: -AwoiGR6c8M_10_14\n  Caption: a boy is playing a piano\n  Sentiment: POSITIVE (Confidence: 0.994)\n\nVideo: -Cv5LsqKUXc_17_25\n  Caption: a woman is removing a stem of strawberry\n  Sentiment: NEGATIVE (Confidence: 0.966)\n\nVideo: -Cv5LsqKUXc_71_76\n  Caption: a woman is filtering some food powder\n  Sentiment: NEGATIVE (Confidence: 0.906)\n\nVideo: -DKuLXYoY3g_14_20\n  Caption: a young boy is bouncing a basketball\n  Sentiment: NEGATIVE (Confidence: 0.813)\n\nVideo: -DRy7rBg0IQ_31_37\n  Caption: a woman is swimming underwater\n  Sentiment: POSITIVE (Confidence: 0.911)\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# ✅ Streamlit App for Video Caption + Sentiment (Fixed for .keras models)\n\nimport streamlit as st\nimport numpy as np\nimport cv2\nimport tempfile\nfrom transformers import pipeline\nimport tensorflow as tf\nimport pickle\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# ✅ Load tokenizer and model (.keras)\ntokenizer = pickle.load(open(\"tokenizer.pkl\", \"rb\"))\nmodel = tf.keras.models.load_model(\"video_caption_model.keras\")\n\n# Load HuggingFace sentiment pipeline\nsentiment_analyzer = pipeline(\"sentiment-analysis\")\n\n# ✅ Caption generation function (fixed)\ndef generate_caption(model, tokenizer, video_feat, max_length=47):  # 47 is your dataset max_length\n    in_text = 'startseq'\n    for _ in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n        yhat = model.predict([np.array([video_feat]), sequence], verbose=0)\n        yhat = np.argmax(yhat)\n        word = tokenizer.index_word.get(yhat)\n        if word is None:\n            break\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n    return in_text.replace('startseq', '').replace('endseq', '').strip()\n\n# ✅ Streamlit UI\nst.title(\"🎥 Video Caption + Sentiment Analyzer\")\n\nuploaded_file = st.file_uploader(\"Upload a Video (.avi)\", type=[\"avi\"])\n\nif uploaded_file is not None:\n    # Save to temp file\n    tfile = tempfile.NamedTemporaryFile(delete=False)\n    tfile.write(uploaded_file.read())\n    video_path = tfile.name\n\n    # Extract frames (5 frames)\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    for i in range(5):\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.resize(frame, (299, 299))\n        frame = tf.keras.applications.inception_v3.preprocess_input(frame)\n        frames.append(frame)\n    cap.release()\n    frames = np.array(frames)\n\n    # Load InceptionV3 model\n    base_model = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n    features = base_model.predict(frames)\n    video_feat = np.mean(features, axis=0)\n\n    # ✅ Generate caption\n    caption = generate_caption(model, tokenizer, video_feat)\n\n    # ✅ Analyze sentiment\n    sentiment = sentiment_analyzer(caption)[0]\n\n    st.success(f\"📜 Caption: {caption}\")\n    st.info(f\"💬 Sentiment: {sentiment['label']} (Confidence: {round(sentiment['score'], 2)})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:32:48.227127Z","iopub.execute_input":"2025-05-11T17:32:48.227809Z","iopub.status.idle":"2025-05-11T17:32:48.282886Z","shell.execute_reply.started":"2025-05-11T17:32:48.227776Z","shell.execute_reply":"2025-05-11T17:32:48.281937Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1586190933.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# ✅ Load tokenizer and model (.keras)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"video_caption_model.keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Load HuggingFace sentiment pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    196\u001b[0m         )\n\u001b[1;32m    197\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0;34mf\"File not found: filepath={filepath}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;34m\"Please ensure the file is an accessible `.keras` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: File not found: filepath=video_caption_model.keras. Please ensure the file is an accessible `.keras` zip file."],"ename":"ValueError","evalue":"File not found: filepath=video_caption_model.keras. Please ensure the file is an accessible `.keras` zip file.","output_type":"error"}],"execution_count":27},{"cell_type":"code","source":"# Phase 6: Sentiment and Topic Classification on Generated Captions\n!pip install transformers torch --quiet\n\nfrom transformers import pipeline\nfrom collections import Counter\n\n# Initialize sentiment and topic pipelines\nsentiment_analyzer = pipeline('sentiment-analysis')\nclassifier = pipeline(\"zero-shot-classification\", model=\"microsoft/deberta-large-mnli\")\nlabels = [\n  \"education\", \"lifestyle\", \"health\", \"travel\", \"politics\",\n  \"finance\", \"technology\", \"sports\", \"entertainment\", \"music\",\n  \"nature\", \"food\", \"animals\", \"vehicles\", \"shopping\", \"weather\",\n  \"fitness\", \"art\", \"culture\", \"history\", \"science\", \"family\",\n  \"transportation\", \"war\", \"crime\", \"disaster\", \"fashion\", \n  \"space\", \"architecture\", \"religion\", \"military\", \"news\", \n  \"environment\", \"economy\", \"real estate\", \"gaming\", \"internet\", \"meditation\"\n]\n\n# Analyze generated captions\ncaption_outputs = []\ntopics = []\nsentiments = []\n\nfor vid in video_features:\n    gen_caption = generate_caption(model, tokenizer, video_features[vid], max_length)\n    caption_outputs.append((vid, gen_caption))\n\n    # sentiment_result = sentiment_analyzer(gen_caption)[0]\n    # sentiments.append(sentiment_result['label'])\n\n    topic_result = classifier(gen_caption, labels)\n    top_label = topic_result['labels'][0]\n    topics.append(top_label)\n\n# Display results\nprint(\"\\nGenerated Captions with Sentiment and Topic:\")\nfor i, (vid, cap) in enumerate(caption_outputs):\n    print(f\"Video: {vid}\\nCaption: {cap}\\nTopic: {topics[i]}\\n\")\n\n# Summary statistics\ntopic_counts = Counter(topics)\ntop_3 = topic_counts.most_common(3)\nprint(\"Top 3 Topics:\")\nfor topic, count in top_3:\n    print(f\"{topic}: {count}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:50:07.685213Z","iopub.execute_input":"2025-05-11T17:50:07.685586Z","iopub.status.idle":"2025-05-11T17:50:38.612887Z","shell.execute_reply.started":"2025-05-11T17:50:07.685558Z","shell.execute_reply":"2025-05-11T17:50:38.612079Z"}},"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nDevice set to use cuda:0\nSome weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nDevice set to use cuda:0\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"\nGenerated Captions with Sentiment and Topic:\nVideo: -4wsuPCjDBc_5_15\nCaption: a squirrel is eating a peanut\nTopic: food\n\nVideo: -7KMZQEsJW4_205_208\nCaption: a man is holding dead sunflowers\nTopic: environment\n\nVideo: -8y1Q0rA3n8_108_115\nCaption: a man is cutting a bottle of water with a sword\nTopic: crime\n\nVideo: -8y1Q0rA3n8_95_102\nCaption: a man is stabbing a cardboard cutout with a sword\nTopic: crime\n\nVideo: -9CUm-2cui8_39_44\nCaption: a woman is boiling finger in a pot\nTopic: food\n\nVideo: -AwoiGR6c8M_10_14\nCaption: a boy is playing a piano\nTopic: music\n\nVideo: -Cv5LsqKUXc_17_25\nCaption: a woman is removing a stem of strawberry\nTopic: food\n\nVideo: -Cv5LsqKUXc_71_76\nCaption: a woman is filtering some food powder\nTopic: food\n\nVideo: -DKuLXYoY3g_14_20\nCaption: a young boy is bouncing a basketball\nTopic: sports\n\nVideo: -DRy7rBg0IQ_31_37\nCaption: a woman is swimming underwater\nTopic: environment\n\nTop 3 Topics:\nfood: 4\nenvironment: 2\ncrime: 2\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}